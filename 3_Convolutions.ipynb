{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "import tensorflow.contrib.layers as layers\n",
    "import os\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To infinity and beyond \n",
    "So far we used low level tensorflow tunctions. Now we going to learn what convolutional neural network is.\n",
    "\n",
    "**Neural Networks** are essentially mathematical models to solve an optimization problem. They are made of neurons, the basic computation unit of neural networks. A neuron takes an input(say x), do some computation on it(say: multiply it with a variable w and adds another variable b ) to produce a value (say; z= wx+b). This value is passed to a non-linear function called activation function(f) to produce the final output(activation) of a neuron.\n",
    "\n",
    "**Layers**\n",
    "If you stack neurons in a single line, itâ€™s called a  layer; which is the next building block of neural networks\n",
    "\n",
    "**Layer types**\n",
    "1. Fully connected layers - dot product with input tensor\n",
    "2. Convolutional layers - apply convolution operation to input tensor\n",
    "2. Pooling layers - reduce spatial dimensions of input tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Let's look more in layers\n",
    "Fully connected layers follow this pattern\n",
    "\n",
    "![alt text]( http://cs231n.github.io/assets/nn1/neural_net2.jpeg \"Fully connected network\")\n",
    "\n",
    "\n",
    "A ConvNet arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels).\n",
    "\n",
    "![alt text]( http://cs231n.github.io/assets/cnn/cnn.jpeg \"Convolutional network\")\n",
    "\n",
    "\n",
    "# CNN neuron connectivity pattern\n",
    "In this example input volume in red (e.g. a 32x32x3 CIFAR-10 image), and an example volume of neurons in the first Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels\n",
    "\n",
    "![alt text](http://cs231n.github.io/assets/cnn/depthcol.jpeg \"Neuron example\")\n",
    "\n",
    "For example, suppose that the input volume has size [32x32x3], (e.g. an RGB CIFAR-10 image). If the receptive field (or the filter size) is 5x5, then each neuron in the Conv Layer will have weights to a [5x5x3] region in the input volume, for a total of 5*5*3 = 75 weights (and +1 bias parameter). Notice that the extent of the connectivity along the depth axis must be 3, since this is the depth of the input volume.\n",
    "\n",
    "# Pooling layers\n",
    "pooling layer reduce tensor spatial dimesion. Max pooling choose max value from pooling region. Mean pooling averages poolng region.\n",
    "\n",
    "![alt text](https://i2.wp.com/cv-tricks.com/wp-content/uploads/2017/02/maxpool.jpg?resize=300%2C140 \"Pooling example\")\n",
    "\n",
    "\n",
    "# CNN architectures \n",
    "\n",
    "cnn architecture tries to answer question: how to assemble layers in model. Basicaly we do this:\n",
    "\n",
    "**INPUT -> [[CONV -> NONL]\\*N -> POOL]\\*M -> [FC->NONL]\\*K -> FC**\n",
    "Where N,M,K are some constants.\n",
    "\n",
    "**Popular architectures**\n",
    "1. GoogLeNet aka Inception(v1, v2, v3, v4)\n",
    "2. VGG\n",
    "3. ResNet\n",
    "4. and many more\n",
    "\n",
    "Look at my presentation if you want to get overview about popular CNN architectures:  https://youtu.be/H-iHcz89M5U?t=32\n",
    "\n",
    "If you want good course on CNN's look at Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition http://cs231n.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look more in convolutions in tensorflow\n",
    "\n",
    "**Low level interface**:\n",
    "```python\n",
    "tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "tf.nn.max_pool(value, ksize, strides, padding, data_format='NHWC', name=None)\n",
    "tf.matmul(X, w) + b - fully connected\n",
    "```\n",
    "requires manually define weights variables, you need to know what are you doing :D, not suitable for rapid prototyping\n",
    "\n",
    "\n",
    "**High level interface**:\n",
    "```python\n",
    "tf.contrib.layers\n",
    "tf.layers \n",
    "```\n",
    "Developers are including code from contrib to core so there is duplication. Tensorflow is changing rapidly so be ready to change your code ir you update tensorflow\n",
    "\n",
    "```python\n",
    "tf.contrib.layers.conv2d(*args, **kwargs)\n",
    "\n",
    "* inputs: a Tensor of rank N+2 of shape [batch_size] + input_spatial_shape + [in_channels]\n",
    "* num_outputs: integer, the number of output filters.\n",
    "* kernel_size: a sequence of N positive integers specifying the spatial dimensions of of the filters. \n",
    "* stride: a sequence of N positive integers specifying the stride at which to compute output.\n",
    "* padding: one of \"VALID\" or \"SAME\".\n",
    "* activation_fn: activation function, set to None to skip it and maintain a linear activation.\n",
    "\n",
    "tf.contrib.layers.fully_connected(*args, **kwargs)\n",
    "\n",
    "* inputs: A tensor of with at least rank 2 and value for the last dimension, i.e. [batch_size, depth]\n",
    "* num_outputs: Integer or long, the number of output units in the layer.\n",
    "* activation_fn: activation function, set to None to skip it and maintain a linear activation.\n",
    "\n",
    "tf.contrib.layers.max_pool2d(*args, **kwargs)\n",
    "\n",
    "* inputs: A 4-D tensor of shape [batch_size, height, width, channels] \n",
    "* kernel_size: A list of length 2: [kernel_height, kernel_width] of the pooling kernel over which the op is computed. \n",
    "* stride: A list of length 2: [stride_height, stride_width]. Can be an int if both strides are the same. Note that * presently both strides must have the same value.\n",
    "* padding: The padding method, either 'VALID' or 'SAME'.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build our CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_model/conv1/weights 0.00079708\n",
      "cnn_model/conv1/biases 0.0\n",
      "cnn_model/conv2/weights -0.000188029\n",
      "cnn_model/conv2/biases 0.0\n",
      "cnn_model/fc1/weights 1.8262e-06\n",
      "cnn_model/fc1/biases 0.0\n",
      "cnn_model/fc2/weights -0.000754143\n",
      "cnn_model/fc2/biases 0.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "def build_models(input):\n",
    "    #Network pattern\n",
    "    #INPUT -> [[CONV -> RELU] -> POOL]*2 -> [FC->RELU] -> FC\n",
    "    \n",
    "    # tensors to tensorflow by default go in by b01c order [batch, height, width, channels]\n",
    "    with tf.variable_scope('cnn_model'):\n",
    "        images = tf.reshape(input, shape=[-1, 28, 28, 1])\n",
    "        net = layers.conv2d(images, 32, [3,3], padding='same', activation_fn=tf.nn.relu, scope='conv1')\n",
    "        net = layers.max_pool2d(net, kernel_size = [2,2], stride=[2,2], scope='pool1')\n",
    "        net = layers.conv2d(net, 64, [3,3], padding='same', activation_fn=tf.nn.relu, scope='conv2')\n",
    "        net = layers.max_pool2d(net, kernel_size = [2,2], stride=[2,2], scope='pool2') #shape is[?,7,7,64]\n",
    "        # To fully connected layer we need to pass 2d tensor\n",
    "        net = layers.flatten(net)\n",
    "        net = layers.fully_connected(net, 512, activation_fn=tf.nn.relu, scope='fc1')\n",
    "        logits = layers.fully_connected(net, 10, activation_fn=None, scope='fc2')\n",
    "        return logits\n",
    "\n",
    "    \n",
    "#Lets look how our model looks like\n",
    "X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "net = build_models(X)\n",
    "\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in tf.global_variables():\n",
    "        print i.op.name, tf.reduce_mean(i).eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets build different model\n",
    "tf.reset_default_graph()\n",
    "def build_model_dropout(input, is_training=True):\n",
    "    DROPOUT = 0.75\n",
    "    # tensors to tensorflow by default go in by b01c order [batch, height, width, channels]\n",
    "    with tf.variable_scope('cnn_model'):\n",
    "        images = tf.reshape(input, shape=[-1, 28, 28, 1])\n",
    "        net = layers.conv2d(images, 32, [3,3], padding='same', activation_fn=tf.nn.relu, scope='conv1')\n",
    "        net = layers.max_pool2d(net, kernel_size = [2,2], stride=[2,2], scope='pool1')\n",
    "        net = layers.conv2d(net, 64, [3,3], padding='same', activation_fn=tf.nn.relu, scope='conv2')\n",
    "        net = layers.max_pool2d(net, kernel_size = [2,2], stride=[2,2], scope='pool2') #shape is[?,7,7,64]\n",
    "        # To fully connected layer we need to pass 2d tensor\n",
    "        net = layers.flatten(net)\n",
    "        net = layers.fully_connected(net, 512, activation_fn=tf.nn.relu, scope='fc1')\n",
    "        net = layers.dropout(net, DROPOUT,is_training=is_training)\n",
    "        logits = layers.fully_connected(net, 10, activation_fn=None, scope='fc2')\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we need to write new train script\n",
    "In your spare time you can think about what other architectures you can think of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:  n_batches per epoch: 429 initial_step: 4200\n",
      "Optimization Finished!\n",
      "Total time: 0.838398933411 seconds\n",
      "Accuracy 0.977400124073\n",
      "./MNIST_data/test/cnn/cnn_model-3799\n",
      "./MNIST_data/test/cnn/cnn_model-3899\n",
      "./MNIST_data/test/cnn/cnn_model-3999\n",
      "./MNIST_data/test/cnn/cnn_model-4099\n",
      "./MNIST_data/test/cnn/cnn_model-4199\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Step 1: Get data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "# Step 2: Define paramaters for the model\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 128\n",
    "SKIP_STEP = 100\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Step 3: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# We'll be doing dropout for hidden layer so we'll need a placeholder\n",
    "# for the dropout probability too\n",
    "# Use None for shape so we can change the batch_size once we've built the graph\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "    \n",
    "    is_training = tf.placeholder(tf.bool, [], name=\"Train_flag\")\n",
    "    \n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "# Step 4: define model\n",
    "# Our model is conv -> relu -> pool -> conv -> relu -> pool -> fully connected -> softmax\n",
    "logits = build_models(X)\n",
    "# logits = build_model_dropout(X)\n",
    "\n",
    "# Step 5: define loss function\n",
    "# use softmax cross entropy with logits as the loss function form tf.losses\n",
    "with tf.name_scope('loss'):\n",
    "    tf.losses.softmax_cross_entropy(y, logits=logits)\n",
    "    loss = tf.losses.get_total_loss(add_regularization_losses=False)\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of LEARNING_RATE to minimize cost\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss, \n",
    "                                        global_step=global_step)\n",
    "# Step 7 define test op\n",
    "with tf.name_scope('test_op'):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Step 8: We create dir where to store checkpoint files\n",
    "save_dir = './MNIST_data/test/cnn/'\n",
    "if not exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    initial_step = global_step.eval()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "    print \"INFO:  n_batches per epoch: {} initial_step: {}\".format(n_batches, initial_step)\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, y:Y_batch}) \n",
    "#         _, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, y:Y_batch, is_training:True}) \n",
    "        total_loss += loss_batch\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.3f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, save_dir+'cnn_model', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    \n",
    "#     acc =  sess.run(accuracy, feed_dict={X: mnist.test.images, y: mnist.test.labels, is_training:False})\n",
    "    acc =  sess.run(accuracy, feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "    print 'Accuracy {0}'.format(acc) \n",
    "    \n",
    "    #Lets list all checkpoint files\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    for saved_model in ckpt.all_model_checkpoint_paths:\n",
    "        print saved_model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So boys and girls now you realy know kung fu :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
